\part{Deep Learning}

\section{Neural Networks}

\subsection{Dropout}
The dropout method has been used for a long time in order to avoid over-fitting. Networks nodes (i.e. neurons) are disable randomly during training time so that with each training step, a different subset of the network architecture is evaluated and adjusted. This can be seen as a regularization and proves to result in better generalization in many cases.

\subsection{Dropout and scaling}
Setting a dropout rate greater than 0, will change the magnitude of the values passed through the network. Consider a simple example of two layers of size 10 and 1. If all activations are approximately 1, the output should be around 10. Now, if we add a dropout with rate 0.5, the output will be around 5. This means that the activation will be too big at test time and need to be rescaled. To avoid an additional operation at test time, this is done at training time with a scaling factor of $\frac{1}{1-p}$.


\subsection{Dropout Sampling (Monte-Carlo)}
Randomly turns off network node during inference. This results in a random prediction with some complex probability distributions every time an input value is passed to the network. The empirical distribution or parameter estimates can be used to obtain, for example, a mean value and a confidence measure in terms of the distributional variance. We expect that the empirical variance is low here there was an abundance of training data since all network subsets had the opportunity to learn in these areas. However, in areas where there was no training data to learn from, the network behavior is not controllable so we expect a high variance among the different network subsets.


\subsection{Deep Ensembles}
in a general regression setup, the neural network takes a vector of inputs and generates a single output that represents our prediction. This is done according the network parameters obtained during training process. However ,if we assume our real data to behave according to a parametric probability distribution whose parameters depend on the input values, we can take the underlying model into account and estimate not only the actual output value but a set of distributional parameters.

Assume that our model has a single output $y$ which is not deterministic but normally distributed with parameters $(\mu(x), \sigma^2(x)$ depending on the input $x$. In the training, instead of using the common mean squared error loss function, we will take into account the distribution as well. We can achieve this by using a maximum likelihood (ML) approach. We can take the negative log-likelihood function of the normal distribution as a loss function (ignoring the constants).

\begin{equation}
    \mathcal{L}(x, y) = -log \phi(y|x) = \frac{log \hat{\sigma}^2(x)}{2}+\frac{(y-\hat{\mu}(x))^2}{2\hat{\sigma}^2(x)}
\end{equation}

And for multiple samples, we average to minimize the \textit{mean negative log-likelihood}.
intuitively, the numerator of the right term encourages the mean prediction $\hat{\mu}(x)$ to be close to the observed value. The denominator term ensures that the variance $\hat{\sigma}^2(x)$ is large when the mean prediction is far from the observed data.

\subsection{Ensemble Averaging}

Instead of training a single network, the idea is to train an ensemble of $M$ networks with different initialization. We expect that all the networks behave similarly in areas with sufficient training data and give completely different results where there is no data available.
For the final prediction, they combine all the results of the networks into a Gaussian mixture distribution, from which it is possible to extract a single mean and variances estimations.

\subsection{Dropout Ensembles}
Combine Dropout and Deep Ensembles.

\subsection{Quantile Regression}
Another classic method for distribution estimation with neural networks.

\subsection{Gaussian Process}
A Gaussian Process is a random function that is defined by its mean and covariance functions.

\section{Bayesian Deep Learning}

\subsection{Two kind of uncertainties}
In Bayesian modeling, there are two main kinds of uncertainties that we can model: \textit{epistemic} and \textit{aleatoric} uncertainty.

\paragraph{Epistemic uncertainty} accounts for uncertainty in the model parameters which captures the ignorance of the model for a certain input data. This uncertainty can be reduced if more training data are given.

\paragraph{Aleatoric uncertainty} captures the noise in the input data, for example, sensor noise. This type of noise cannot be reduced even if more data were collected.
Aleatoric uncertainty can further be categorized into:
\begin{itemize}
    \item \textit{homoscedastic uncertainty}, uncertainty which stays constant for different inputs
    \item \textit{heteroscedastic uncertainty}, uncertainty which depends on the input data
\end{itemize}

\section{Layers}
\subsection{Conv2d}
The spatial size of the input changes as follows:

\begin{equation}
    H_{out} = \left\lfloor \frac{H_{in}+2\times padding[0] - dilation[0]\times (kernel\_size[0]-1)-1}{stride[0]}+1 \right\rfloor
\end{equation}
\begin{equation}
    W_{out} = \left\lfloor \frac{W_{in}+2\times padding[1]-dilation[1]\times(kernel\_size[1]-1)-1}{stride[1]}+1 \right\rfloor
\end{equation}

\section{Loss Functions}

\subsection{L1 Loss}
It creates a criterion that measures the mean absolute error (MAE) between the input $x$ and the target $y$.
\begin{equation}
    L = \frac{1}{N} \sum_{i=0}^{N} = |x_i - y_i|
\end{equation}

\subsection{MSE Loss}
It creates a criterion that measures the mean squared error between the input $x$ and the target $y$.
\begin{equation}
    L =  \frac{1}{N} \sum_{i=0}^{N} = (x_i - y_i)^2
\end{equation}

\subsection{Cross-entropy Loss}
This loss function is used for classification. It combines the \textit{Log Softmax} and the \textit{NLL Loss} into a single loss. 
This criterion expects a class index $class$ as target value and the input $x$ should contain raw, unnormalized scores for each class.
\begin{equation}
    loss(x, class) = -\log \left(\frac{\exp(x_{class})}{\sum_j \exp(x_j)}\right)
\end{equation}
The losses are averaged across observations for each minibatch.
\begin{equation}
    L = \frac{1}{N} loss(x_i, class_i)
\end{equation}

\section{Yolo}
Basic idea for the Yolo (v1, v2, v3) object detection algorithm.

\begin{algorithm}[H]
\DontPrintSemicolon
\KwInput{Input image}
\KwOutput{a set of bounding boxes with confidence and label}
 Divide the image in a $S\times S$ grid. \\
\For{each cell}
  { predict: \\
  - B boxes ($c_x, c_y, w, h$ for each anchor box) \\
  - B confidence values (the confidence should be equal to $Pr(Object) \times IoU$, if there is no object in the cell it should be 0 and if there is if should be equal to the IoU with its bbox) \\
  - C class conditional probabilities ($Pr(class_i|Object$) \\}
 The final tensor is $S\times S \times (B \times 5 + C)$ \\
 Threshold and apply non-maximum suppression
\caption{Yolo}
\end{algorithm}


\section{Mask R-CNN}
Basic idea for the Mask R-CNN object detection and segmentation

\begin{algorithm}[H]
\DontPrintSemicolon
\KwInput{Input image}
\KwOutput{a set of bounding boxes + label + mask}
 Apply the \textit{backbone} network to compute feature maps \\
 Apply the RPN (Region Proposal Network) that outputs candidate RoI (objectness scores + box coordinates) \\
 Apply Fast R-CNN on each RoI with an additional branch for the mask:\\
 \For{each RoI}
 {
  apply Fast R-CNN with an additional branch for the mask: \\
 - classification: a set of class scores  \\
 - bbox regression: per-class bbox offsets \\
 - mask prediction: one binary mask per class \tcp*{only the mask corresponding to the true class is used during training}
 }
 \caption{Mask R-CNN}
\end{algorithm}

