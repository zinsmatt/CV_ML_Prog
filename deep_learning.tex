\part{Deep Learning}

\section{Neural Networks}

\subsection{Dropout}
The dropout method has been used for a long time in order to avoid over-fitting. Networks nodes (i.e. neurons) are disable randomly during training time so that with each training step, a different subset of the network architecture is evaluated and adjusted. This can be seen as a regularization and proves to result in better generalization in many cases.

\subsection{Dropout and scaling}
Setting a dropout rate greater than 0, will change the magnitude of the values passed through the network. Consider a simple example of two layers of size 10 and 1. If all activations are approximately 1, the output should be around 10. Now, if we add a dropout with rate 0.5, the output will be around 5. This means that the activation will be too big at test time and need to be rescaled. To avoid an additional operation at test time, this is done at training time with a scaling factor of $\frac{1}{1-p}$.


\subsection{Dropout Sampling (Monte-Carlo)}
Randomly turns off network node during inference. This results in a random prediction with some complex probability distributions every time an input value is passed to the network. The empirical distribution or parameter estimates can be used to obtain, for example, a mean value and a confidence measure in terms of the distributional variance. We expect that the empirical variance is low here there was an abundance of training data since all network subsets had the opportunity to learn in these areas. However, in areas where there was no training data to learn from, the network behavior is not controllable so we expect a high variance among the different network subsets.


\subsection{Deep Ensembles}
in a general regression setup, the neural network takes a vector of inputs and generates a single output that represents our prediction. This is done according the network parameters obtained during training process. However ,if we assume our real data to behave according to a parametric probability distribution whose parameters depend on the input values, we can take the underlying model into account and estimate not only the actual output value but a set of distributional parameters.

Assume that our model has a single output $y$ which is not deterministic but normally distributed with parameters $(\mu(x), \sigma^2(x)$ depending on the input $x$. In the training, instead of using the common mean squared error loss function, we will take into account the distribution as well. We can achieve this by using a maximum likelihood (ML) approach. We can take the negative log-likelihood function of the normal distribution as a loss function (ignoring the constants).

\begin{equation}
    \mathcal{L}(x, y) = -log \phi(y|x) = \frac{log \hat{\sigma}^2(x)}{2}+\frac{(y-\hat{\mu}(x))^2}{2\hat{\sigma}^2(x)}
\end{equation}

And for multiple samples, we average to minimize the \textit{mean negative log-likelihood}.
intuitively, the numerator of the right term encourages the mean prediction $\hat{\mu}(x)$ to be close to the observed value. The denominator term ensures that the variance $\hat{\sigma}^2(x)$ is large when the mean prediction is far from the observed data.

\subsection{Ensemble Averaging}

Instead of training a single network, the idea is to train an ensemble of $M$ networks with different initialization. We expect that all the networks behave similarly in areas with sufficient training data and give completely different results where there is no data available.
For the final prediction, they combine all the results of the networks into a Gaussian mixture distribution, from which it is possible to extract a single mean and variances estimations.

\subsection{Dropout Ensembles}
Combine Dropout and Deep Ensembles.

\subsection{Quantile Regression}
Another classic method for distribution estimation with neural networks.

\subsection{Gaussian Process}
A Gaussian Process is a random function that is defined by its mean and covariance functions.

\section{Bayesian Deep Learning}

\subsection{Two kind of uncertainties}
In Bayesian modeling, there are two main kinds of uncertainties that we can model: \textit{epistemic} and \textit{aleatoric} uncertainty.

\paragraph{Epistemic uncertainty} accounts for uncertainty in the model parameters which captures the ignorance of the model for a certain input data. This uncertainty can be reduced if more training data are given.

\paragraph{Aleatoric uncertainty} captures the noise in the input data, for example, sensor noise. This type of noise cannot be reduced even if more data were collected.
Aleatoric uncertainty can further be categorized into:
\begin{itemize}
    \item \textit{homoscedastic uncertainty}, uncertainty which stays constant for different inputs
    \item \textit{heteroscedastic uncertainty}, uncertainty which depends on the input data
\end{itemize}

\subsection{Weight Uncertainty in Neural Networks}
\subsubsection{Terminology}

\begin{equation}
    P(\mathbf{w}|\mathcal{D}) = \frac{P(\mathcal{D}|\mathbf{w})P(\mathbf{w})}{P(\mathcal{D})} = \frac{P(\mathcal{D}|\mathbf{w})P(\mathbf{w})}{\int P(\mathcal{D}|\mathbf{w})P(\mathbf{w}) d\mathbf{w}}
\end{equation}

\begin{itemize}
    \item $\mathbf{w}$ are the weights of the network.
 \item $\mathcal{D}$ is the training data.
\item $P(\mathbf{w}|\mathcal{D})$ is the \textbf{posterior} probability of $\mathbf{w}$. This is the probability distribution of the weights given an observed set of data.
\item $P(\mathcal{D}|\mathbf{w})$ is the \textbf{likelihood} of $\mathbf{w}$. This is the probability distribution of the data given fixed weights.
\item $P(\mathbf{w})$ is the \textbf{prior} probability on $\mathbf{w}$. This represents our initial beliefs on the distribution of the weights, before observing any data.
\item $P(\mathcal{D})$ is the \textbf{evidence} (also called marginal likelihood) of the data.
\end{itemize}


Log-probabilities are often used instead because they provide better numerical stability. Probabilities are usually small numbers and multiplication of them can be imprecise. Taking the log gives large negative values and transforms products into additions.


\subsubsection{Bayesian Inference and Prediction}

A neural network can be viewed as a probabilistic model $P(y|x, \mathbf{w})$. In case of classification, this corresponds to the softmax output.

The normal approach of training a neural network by updating weights using gradient descent seeks to find the weights which best explain the data. This can be seen as learning the weights which maximize the likelihood $P(\mathcal{D}|\mathbf{w})$, through \textbf{Maximum Likelihood Estimation (MLE)}.

\begin{align}
    \mathbf{w}^{MLE} &= \arg\max_{\mathbf{w}} P(\mathcal{D}|\mathbf{w}) \\
    & = \arg\max_{\mathbf{w}} \prod_i P(y_i|x_i, \mathbf{w})
\end{align}

This is the frequentist perspective, where the weights are fixed and the data is viewed as a random variable. 
Instead, we can view the data as being fixed and the model weights as being a random variable. We can train to maximize the posterior $P(\mathbf{w}|\mathcal{D})$ via \textbf{Maximum a Posteriori (MAP)} learning. This is equivalent to MLE objective  with an additional regularization term using the prior distribution of the weights:

\begin{align}
    \mathbf{w}^{MAP} &= \arg\max_{\mathbf{w}} P(\mathbf{w}|\mathcal{D}) \\
    & = \arg\max_{\mathbf{w}} P(\mathcal{D}|\mathbf{w})P(\mathbf{w}) \\
    & = \arg\max_{\mathbf{w}} \log P(\mathcal{D}|\mathbf{w}) + \log P(\mathbf{w})
\end{align}

Having obtained the MLE or MAP weights, prediction can be simply using the model with the learned weights fixed:

\begin{equation}
    P(\hat{y}|\hat{x}) = P(\hat{y}|\hat{x}, \mathbf{w}^{MLE})
\end{equation}


If we consider the entire posterior distribution of the weights $P(\mathbf{w}|\mathcal{D})$, the predictive distribution becomes a weighted expectation over all possible values for $\mathbf{w}$.

\begin{align}
    P(\hat{y}|\hat{x}) & = \mathbb{E}_{p(\mathbf{w}|\mathcal{D})}[P(\hat{y}|\hat{x}, \mathbf{w})] \\
    & = \int P(\hat{y}|\hat{x}, \mathbf{w})P(\mathbf{w}|\mathcal{D}) d\mathbf{w}
\end{align}

\subsubsection{Variational Inference}
Unfortunately, this predictive distribution is generally intractable for neural networks. Also, even if the prior $P(\mathbf{w})$ is something that we can choose and the likelihood $P(\mathcal{D}|\mathbf{w})$ can be computed, the posterior $P(\mathbf{w}|\mathcal{D})$ is intractable. More precisely, the evidence $P(\mathcal{D})$ in the denominator requires integration over the very high-dimensional space of weights and has no analytical solution:

\begin{align}
    P(\mathcal{D}) & = \int P(\mathcal{D}, \mathbf{w}) d\mathbf{w} \\
    & = \int P(\mathcal{D}|\mathbf{w}) P(\mathbf{w}) d\mathbf{w}
\end{align}


Thus, we need to employ approximations. \textbf{Variational inference} constructs a new distribution $q(\mathbf{w}|\theta)$ over the weights $\mathbf{w}$ and parameterized by $\theta$, that approximates the true posterior $P(\mathbf{w}|\mathcal{D})$. It finds the parameters $\theta$ which minimizes the \textit{KL}-divergence between $q$ and $P$.
\begin{equation}
    \theta^* = \arg\min_{\theta}KL[q(\mathbf{w}|\theta) || P(\mathbf{w}|\mathcal{D}]
\end{equation}

The KL-divergence is an information-theoric measure of the difference between distributions. This is not a true distance metric because it is not symmetric.

\begin{equation}
    KL[q(x)||P(x)] = \int q(x) \log \frac{q(x)}{P(x)}dx
\end{equation}

We substitute this into the previous equation and reformulat it to only depend on values we know:

\begin{align}
    \theta^* &= \arg\min_{\theta} \int q(\mathbf{w}|\theta) \log \frac{q(\mathbf{w}|\theta)}{P(\mathbf{w}|\mathcal{D})}d\mathbf{w} \\
    &= \arg\min_{\theta} \int q(\mathbf{w}|\theta) \log \frac{q(\mathbf{w}|\theta)}{P(\mathcal{D}|\mathbf{w})P(\mathbf{w})} d\mathbf{w}
\end{align}

Directly from this, we can construct a cost function which will seek the minimum setting of $\theta$ for:

\begin{align}
    \mathcal{F}(\mathcal{D}, \theta) &= \int q(\mathbf{w}|\theta) \log \frac{q(\mathbf{w}|\theta)}{P(\mathbf{w})} - q(\mathbf{w}|\theta)\log P(\mathcal{D}|\mathbf{w})d\mathbf{w} \\
    &= KL[q(\mathbf{w}|\theta)||P(\mathbf{w})] - \mathbb{E}_{q(\mathbf{w}|\theta)}[\log P(\mathcal{D}|\mathbf{w})]
\end{align}

This cost is a balance between having a variational posterior that is close tho the prior and also be able to explain the complexity of the data. The negative of this cost $-F$ is also called \textbf{Evidence Lower Bound (ELBO)}. Thus minimizing this cost function is equivalent to maximizing the ELBO. It's important to note that the variational inference is known to underestimate the uncertainty of models.


\subsubsection{Bayes by Backprop}

Calculting the expectation of the likelihood over the variational posterior is computationally prohibitive. Once more, we need to use approximation. We approximate our cost function using sampled weights:

\begin{equation}
    \mathcal{F}(\mathcal{D}, \theta) \approx \sum_{i=1}^n \log q(\mathbf{w}^{(i)}|\theta) - \log P(\mathbf{w}^{(i)}) - \log P(\mathcal{D}|\mathbf{w}^{(i)})
\end{equation} 

Using automatic differentiation as provided by frameworks such as PyTorch, we only need to worry about implementing this sampling, and setting up the cost function as above. The usual backpropagation methods can be used to train the model.

\section{Layers}
\subsection{Conv2d}
The spatial size of the input changes as follows:

\begin{equation}
    H_{out} = \left\lfloor \frac{H_{in}+2\times padding[0] - dilation[0]\times (kernel\_size[0]-1)-1}{stride[0]}+1 \right\rfloor
\end{equation}
\begin{equation}
    W_{out} = \left\lfloor \frac{W_{in}+2\times padding[1]-dilation[1]\times(kernel\_size[1]-1)-1}{stride[1]}+1 \right\rfloor
\end{equation}

\subsection{Softmax}
Applying Softmax rescales the element of a tensor so that they range in [0, 1] and sum to 1. The output of this layer is often interpreted as probabilities.
\begin{equation}
    Softmax(x_i) = \frac{exp(x_i)}{\sum_j exp(x_j)}
\end{equation}

\subsection{LogSoftmax}
Applying LogSoftmax additionally applies log to a softmax. The output can be interpreted as log-probabilities.
\begin{equation}
    LogSoftmax(x_i) = log\left(\frac{exp(x_i)}{\sum_j exp(x_j)}\right)
\end{equation}
\section{Loss Functions}

\subsection{L1 Loss}
It creates a criterion that measures the mean absolute error (MAE) between the input $x$ and the target $y$.
\begin{equation}
    L = \frac{1}{N} \sum_{i=0}^{N} = |x_i - y_i|
\end{equation}

\subsection{MSE Loss}
It creates a criterion that measures the mean squared error between the input $x$ and the target $y$.
\begin{equation}
    L =  \frac{1}{N} \sum_{i=0}^{N} = (x_i - y_i)^2
\end{equation}

\subsection{Cross-entropy Loss}
This loss function is used for classification. It combines the \textit{Log Softmax} and the \textit{NLL Loss} into a single loss. 
This criterion expects a class index $class$ as target value and the input $x$ should contain raw, unnormalized scores for each class.
\begin{equation}
    loss(x, class) = -\log \left(\frac{\exp(x_{class})}{\sum_j \exp(x_j)}\right)
\end{equation}
The losses are averaged across observations for each minibatch.
\begin{equation}
    L = \frac{1}{N} loss(x_i, class_i)
\end{equation}

\subsection{Negative Log-Likelihood Loss (NLL)}
 It is useful to train a classification problem with C classes.
 The input contains the log-probabilities of each class. These can be obtained from a \textit{LogSoftmax} layer.
 
 \begin{equation}
    l_n(x_n, y_n) = -x_{n, y_n} 
 \end{equation}
which is just the negative log-probability of the ground-truth class.
For a batch of size N, the total loss can be:
\begin{equation}
    L = \left\{ \begin{array}{ll}
        \frac{1}{N}\sum_{n=1}^{N} l_n &\;if\;reduction="mean" \\
        \sum_{n=1}^{N} l_n &\;if\;reduction="sum"
    \end{array}\right.
\end{equation}

Optionally, it is possible to specify weights $w_i$ to each class. The loss becomes
\begin{equation}
    l_n(x, y) = -w_{y_n} x_{n, y_n}
\end{equation}
\section{Yolo}
Basic idea for the Yolo (v1, v2, v3) object detection algorithm.

\begin{algorithm}[H]
\DontPrintSemicolon
\KwInput{Input image}
\KwOutput{a set of bounding boxes with confidence and label}
 Divide the image in a $S\times S$ grid. \\
\For{each cell}
  { predict: \\
  - B boxes ($c_x, c_y, w, h$ for each anchor box) \\
  - B confidence values (the confidence should be equal to $Pr(Object) \times IoU$, if there is no object in the cell it should be 0 and if there is if should be equal to the IoU with its bbox) \\
  - C class conditional probabilities ($Pr(class_i|Object$) \\}
 The final tensor is $S\times S \times (B \times 5 + C)$ \\
 Threshold and apply non-maximum suppression
\caption{Yolo}
\end{algorithm}


\section{Mask R-CNN}
Basic idea for the Mask R-CNN object detection and segmentation

\begin{algorithm}[H]
\DontPrintSemicolon
\KwInput{Input image}
\KwOutput{a set of bounding boxes + label + mask}
 Apply the \textit{backbone} network to compute feature maps \\
 Apply the RPN (Region Proposal Network) that outputs candidate RoI (objectness scores + box coordinates) \\
 Apply Fast R-CNN on each RoI with an additional branch for the mask:\\
 \For{each RoI}
 {
  apply Fast R-CNN with an additional branch for the mask: \\
 - classification: a set of class scores  \\
 - bbox regression: per-class bbox offsets \\
 - mask prediction: one binary mask per class \tcp*{only the mask corresponding to the true class is used during training}
 }
 \caption{Mask R-CNN}
\end{algorithm}

